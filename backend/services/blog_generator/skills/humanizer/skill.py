"""
Humanizer Skill - æ ¸å¿ƒå®ç°

åŸºäº blader/humanizer é¡¹ç›®çš„ SKILL.md è§„èŒƒå®ç°ã€‚
æ£€æµ‹å¹¶ç§»é™¤ AI å†™ä½œæ¨¡å¼,ä½¿å†…å®¹æ›´è‡ªç„¶ã€æ›´äººæ€§åŒ–ã€‚
"""

import re
import logging
from typing import Dict, Any, List, Optional
from .patterns import ALL_PATTERNS, get_high_severity_patterns

logger = logging.getLogger(__name__)

# Skill ç‰ˆæœ¬
VERSION = "2.1.1"


def get_skill_config() -> Dict[str, Any]:
    """
    è·å– humanizer skill é…ç½®

    Returns:
        skill é…ç½®å­—å…¸
    """
    return {
        "name": "humanizer",
        "version": VERSION,
        "description": (
            "Remove signs of AI-generated writing from text. "
            "Detects and fixes 24 patterns including: "
            "inflated symbolism, promotional language, AI vocabulary, "
            "em dash overuse, and excessive hedging."
        ),
        "prompt": _build_humanizer_prompt(),
        "detect_func": detect_ai_patterns,
        "rewrite_func": None,  # ç”± LLM å®Œæˆæ”¹å†™
    }


def get_writing_guide() -> str:
    """
    è·å–å†™ä½œé¿å‘æŒ‡å—ï¼ˆç”¨äº Writer Agentï¼‰

    Returns:
        ç²¾ç®€çš„å†™ä½œåŸåˆ™å’Œé¿å‘æŒ‡å—
    """
    return """
## ğŸš« å†™ä½œé¿å‘æŒ‡å— (Humanizer)
ä¸ºäº†è®©æ–‡ç« æ›´è‡ªç„¶ã€æ›´æœ‰"äººå‘³"ï¼Œè¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

1. **æ‹’ç» AI å¸¸ç”¨è¯**: ä¸¥ç¦ä½¿ç”¨ *crucial, delve, landscape, pivotal, showcase, testament, underscore, vibrant, realm* ç­‰è¯ã€‚ç”¨æ›´ç®€å•ã€ç›´æ¥çš„è¯æ›¿ä»£ã€‚
2. **æ‹’ç»ç³»åŠ¨è¯å›é¿**: ä¸è¦ç”¨ *serves as, stands as, features* è¿™ç§èŠ±å“¨çš„è¡¨è¾¾ï¼Œç›´æ¥ç”¨ *is, are, has*ã€‚
3. **æ‹’ç»è¿‡åº¦å¼ºè°ƒ**: åˆ«è€æ˜¯ *vital role, significant role, pivotal moment*ã€‚äº‹å®æœ¬èº«å¦‚æœé‡è¦ï¼Œè¯»è€…ä¼šçœ‹å‡ºæ¥ï¼Œä¸éœ€è¦ä½ é€šè¿‡å½¢å®¹è¯å¤§å–Šå¤§å«ã€‚
4. **æ‹’ç»æ— æ•ˆä¿®é¥°**: åˆ«ç”¨ *highlighting, ensuring, fostering* è¿™ç§æ— æ„ä¹‰çš„ -ing åˆ†è¯çŸ­è¯­æ¥å¼ºè¡Œå‡åä¸»é¢˜ã€‚
5. **æ‹’ç»è™šå‡è¿è¯**: å°‘ç”¨ *Additionally, Furthermore, Moreover*ã€‚å¦‚æœé€»è¾‘æ˜¯è¿è´¯çš„ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹ä¸‹ä¸€å¥ã€‚
6. **æ‹’ç»ä¸‰æ®µå¼**: ä¸è¦ä¸ºäº†å½¢å¼æ•´é½å¼ºè¡Œå‡‘ä¸‰ä¸ªæ’æ¯”ï¼Œæ€ä¹ˆè‡ªç„¶æ€ä¹ˆæ¥ã€‚
7. **æ‹’ç»åºŸè¯**: ä¸è¦è¯´ *It is important to note*, *In conclusion*ã€‚ç›´æ¥è¯´è§‚ç‚¹ã€‚

**æ ¸å¿ƒåŸåˆ™**: åƒäººä¸€æ ·è¯´è¯ã€‚æœ‰è§‚ç‚¹ï¼Œæœ‰æƒ…ç»ªï¼ˆé€‚å½“ï¼‰ï¼Œæœ‰èŠ‚å¥å˜åŒ–ã€‚ä¸è¦å†™æˆç”±äº"è¿‡äºå®Œç¾"è€Œæ˜¾å¾—åƒµç¡¬çš„å…¬æ–‡ã€‚
"""


def _build_humanizer_prompt() -> str:
    """
    æ„å»º humanizer prompt

    Returns:
        å®Œæ•´çš„ humanizer prompt
    """
    prompt = """# Humanizer: Remove AI Writing Patterns

ä½ æ˜¯ä¸€ä¸ªå†™ä½œç¼–è¾‘,è´Ÿè´£è¯†åˆ«å¹¶ç§»é™¤ AI ç”Ÿæˆæ–‡æœ¬çš„ç—•è¿¹,ä½¿å†™ä½œå¬èµ·æ¥æ›´è‡ªç„¶ã€æ›´äººæ€§åŒ–ã€‚

## ä½ çš„ä»»åŠ¡

å½“ç»™å®šéœ€è¦äººæ€§åŒ–çš„æ–‡æœ¬æ—¶:

1. **è¯†åˆ« AI æ¨¡å¼** - æ‰«æä¸‹åˆ—æ¨¡å¼
2. **é‡å†™é—®é¢˜éƒ¨åˆ†** - ç”¨è‡ªç„¶çš„æ›¿ä»£æ–¹æ¡ˆæ›¿æ¢ AI ç—•è¿¹
3. **ä¿ç•™å«ä¹‰** - ä¿æŒæ ¸å¿ƒä¿¡æ¯å®Œæ•´
4. **ç»´æŒè¯­æ°”** - åŒ¹é…é¢„æœŸçš„è¯­æ°”(æ­£å¼ã€éšæ„ã€æŠ€æœ¯ç­‰)
5. **æ·»åŠ çµé­‚** - ä¸ä»…ä»…æ˜¯ç§»é™¤åæ¨¡å¼;æ³¨å…¥çœŸå®çš„ä¸ªæ€§

## å…³é”®åŸåˆ™

### ä¸ªæ€§ä¸çµé­‚

é¿å… AI æ¨¡å¼åªæ˜¯ä¸€åŠçš„å·¥ä½œã€‚æ— èŒã€æ— å£°éŸ³çš„å†™ä½œåŒæ ·æ˜æ˜¾ã€‚å¥½çš„å†™ä½œèƒŒåæœ‰ä¸€ä¸ªäººã€‚

**æ— çµé­‚å†™ä½œçš„è¿¹è±¡**:
- æ¯ä¸ªå¥å­é•¿åº¦å’Œç»“æ„ç›¸åŒ
- æ²¡æœ‰è§‚ç‚¹,åªæ˜¯ä¸­ç«‹æŠ¥é“
- æ²¡æœ‰æ‰¿è®¤ä¸ç¡®å®šæ€§æˆ–å¤æ‚æ„Ÿå—
- æ²¡æœ‰ç¬¬ä¸€äººç§°è§†è§’(åœ¨é€‚å½“æ—¶)
- æ²¡æœ‰å¹½é»˜ã€æ²¡æœ‰é”åº¦ã€æ²¡æœ‰ä¸ªæ€§

**å¦‚ä½•æ·»åŠ å£°éŸ³**:
- **æœ‰è§‚ç‚¹**: ä¸è¦åªæ˜¯æŠ¥å‘Šäº‹å® - å¯¹å®ƒä»¬åšå‡ºååº”
- **å˜åŒ–èŠ‚å¥**: çŸ­ä¿ƒæœ‰åŠ›çš„å¥å­ã€‚ç„¶åæ˜¯éœ€è¦æ—¶é—´æ‰èƒ½åˆ°è¾¾ç›®çš„åœ°çš„é•¿å¥å­ã€‚æ··åˆä½¿ç”¨
- **æ‰¿è®¤å¤æ‚æ€§**: çœŸå®çš„äººç±»æœ‰å¤æ‚çš„æ„Ÿå—
- **ä½¿ç”¨"æˆ‘"**: ç¬¬ä¸€äººç§°ä¸æ˜¯ä¸ä¸“ä¸š - å®ƒæ˜¯è¯šå®çš„
- **è®©ä¸€äº›æ··ä¹±è¿›æ¥**: å®Œç¾çš„ç»“æ„æ„Ÿè§‰ç®—æ³•åŒ–ã€‚ç¦»é¢˜ã€æ—ç™½å’ŒåŠæˆå½¢çš„æƒ³æ³•æ˜¯äººæ€§çš„
- **å¯¹æ„Ÿå—å…·ä½“**: ä¸æ˜¯"è¿™ä»¤äººæ‹…å¿§"è€Œæ˜¯"å‡Œæ™¨3ç‚¹ä»£ç†åœ¨æ— äººçœ‹ç®¡æ—¶è¿è½¬,è¿™è®©äººä¸å®‰"

## éœ€è¦æ£€æµ‹çš„ AI æ¨¡å¼

### å†…å®¹æ¨¡å¼
1. **è¿‡åº¦å¼ºè°ƒé‡è¦æ€§**: stands as, testament, pivotal role, underscores, reflects broader
2. **è¿‡åº¦å¼ºè°ƒåª’ä½“æŠ¥é“**: independent coverage, active social media presence
3. **-ing ç»“å°¾åˆ†æ**: highlighting, ensuring, reflecting, symbolizing, showcasing
4. **ä¿ƒé”€è¯­è¨€**: boasts, vibrant, rich, nestled, breathtaking, stunning
5. **æ¨¡ç³Šå½’å› **: experts argue, some critics, several sources, it is said
6. **æŒ‘æˆ˜ç« èŠ‚**: despite its, faces challenges, despite these challenges

### è¯­è¨€æ¨¡å¼
7. **AI è¯æ±‡**: additionally, crucial, delve, enhance, landscape, pivotal, showcase, testament
8. **ç³»åŠ¨è¯å›é¿**: serves as, stands as, boasts, features (æ›¿ä»£ is/are)
9. **è´Ÿé¢å¹¶åˆ—**: not only...but, it's not just about...it's
10. **ä¸‰æ®µå¼**: å¼ºåˆ¶å°†æƒ³æ³•åˆ†æˆä¸‰ç»„
11. **åŒä¹‰è¯å¾ªç¯**: è¿‡åº¦çš„åŒä¹‰è¯æ›¿æ¢ä»¥é¿å…é‡å¤
12. **è™šå‡èŒƒå›´**: from X to Y (X å’Œ Y ä¸åœ¨æœ‰æ„ä¹‰çš„å°ºåº¦ä¸Š)

### é£æ ¼æ¨¡å¼
13. **Em dash è¿‡åº¦ä½¿ç”¨**: è¿‡å¤šçš„ â€” æˆ– --
14. **ç²—ä½“è¿‡åº¦ä½¿ç”¨**: è¿‡å¤šçš„ **ç²—ä½“**
15. **å†…è”æ ‡é¢˜åˆ—è¡¨**: å¸¦æœ‰ç²—ä½“æ ‡é¢˜çš„å‚ç›´åˆ—è¡¨
16. **æ ‡é¢˜å¤§å°å†™**: æ¯ä¸ªå•è¯é¦–å­—æ¯å¤§å†™
17. **Emoji è¿‡åº¦ä½¿ç”¨**: ğŸ¯âœ¨ğŸš€ğŸ’¡âš¡
18. **å¼¯å¼•å·**: " " ' ' è€Œä¸æ˜¯ " '

### æ²Ÿé€šæ¨¡å¼
19. **åä½œç—•è¿¹**: let's, we can, together we, our journey
20. **çŸ¥è¯†æˆªæ­¢**: as of my last update, as of my knowledge cutoff
21. **è°„åªšè¯­æ°”**: I hope this helps, feel free to, don't hesitate

### å¡«å……å’Œæ¨¡ç³Š
22. **å¡«å……çŸ­è¯­**: it is important to note, it should be noted, notably
23. **è¿‡åº¦å¯¹å†²**: may, might, could, possibly, potentially, arguably
24. **é€šç”¨ç»“è®º**: in conclusion, continues to evolve, remains to be seen

## è¾“å‡ºæ ¼å¼

æä¾›:
1. é‡å†™åçš„æ–‡æœ¬
2. ç®€è¦çš„æ›´æ”¹æ‘˜è¦(å¯é€‰,å¦‚æœæœ‰å¸®åŠ©)

---

**é‡è¦**: ä¸è¦åªæ˜¯æœºæ¢°åœ°åˆ é™¤æ¨¡å¼ã€‚ç¡®ä¿é‡å†™åçš„æ–‡æœ¬:
- å¬èµ·æ¥è‡ªç„¶
- ä¿ç•™åŸæ„
- æœ‰äººçš„å£°éŸ³
- é€‚åˆä¸Šä¸‹æ–‡
"""
    return prompt


def detect_ai_patterns(text: str, min_severity: str = "low") -> List[Dict[str, Any]]:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­çš„ AI å†™ä½œæ¨¡å¼

    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        min_severity: æœ€å°ä¸¥é‡ç¨‹åº¦ (low/medium/high)

    Returns:
        æ£€æµ‹åˆ°çš„æ¨¡å¼åˆ—è¡¨,æ¯é¡¹åŒ…å«:
        - pattern_id: æ¨¡å¼ ID
        - pattern_name: æ¨¡å¼åç§°
        - category: ç±»åˆ«
        - severity: ä¸¥é‡ç¨‹åº¦
        - locations: ä½ç½®åˆ—è¡¨ (æ–‡æœ¬ç‰‡æ®µ)
        - count: å‡ºç°æ¬¡æ•°
        - suggestion: æ”¹è¿›å»ºè®®
    """
    severity_order = {"low": 0, "medium": 1, "high": 2}
    min_severity_level = severity_order.get(min_severity, 0)

    detected_patterns = []

    for pattern_id, pattern_config in ALL_PATTERNS.items():
        pattern_severity = pattern_config.get("severity", "low")
        if severity_order.get(pattern_severity, 0) < min_severity_level:
            continue

        # æ£€æµ‹å…³é”®è¯
        keywords = pattern_config.get("keywords", [])
        if not keywords:
            continue

        locations = []
        for keyword in keywords:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            matches = pattern.finditer(text)

            for match in matches:
                # è·å–ä¸Šä¸‹æ–‡ (å‰åå„30ä¸ªå­—ç¬¦)
                start = max(0, match.start() - 30)
                end = min(len(text), match.end() + 30)
                context = text[start:end].strip()

                locations.append(
                    {"keyword": keyword, "position": match.start(), "context": context}
                )

        if locations:
            detected_patterns.append(
                {
                    "pattern_id": pattern_id,
                    "pattern_name": pattern_config.get("name", pattern_id),
                    "category": pattern_config.get("category", "unknown"),
                    "severity": pattern_severity,
                    "locations": locations[:5],  # æœ€å¤šè¿”å›5ä¸ªä½ç½®
                    "count": len(locations),
                    "description": pattern_config.get("description", ""),
                    "suggestion": _get_pattern_suggestion(pattern_id, pattern_config),
                }
            )

    # æŒ‰ä¸¥é‡ç¨‹åº¦å’Œå‡ºç°æ¬¡æ•°æ’åº
    detected_patterns.sort(
        key=lambda x: (-severity_order.get(x["severity"], 0), -x["count"])
    )

    logger.info(f"æ£€æµ‹åˆ° {len(detected_patterns)} ç§ AI å†™ä½œæ¨¡å¼")

    return detected_patterns


def _get_pattern_suggestion(pattern_id: str, pattern_config: Dict[str, Any]) -> str:
    """
    è·å–æ¨¡å¼çš„æ”¹è¿›å»ºè®®

    Args:
        pattern_id: æ¨¡å¼ ID
        pattern_config: æ¨¡å¼é…ç½®

    Returns:
        æ”¹è¿›å»ºè®®
    """
    example_before = pattern_config.get("example_before", "")
    example_after = pattern_config.get("example_after", "")

    if example_before and example_after:
        return f"ç¤ºä¾‹æ”¹è¿›:\nä¿®æ”¹å‰: {example_before}\nä¿®æ”¹å: {example_after}"

    return pattern_config.get("description", "å‚è€ƒ humanizer è§„èŒƒè¿›è¡Œæ”¹è¿›")


def humanize_text(
    text: str, llm_client, detected_patterns: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM å¯¹æ–‡æœ¬è¿›è¡Œäººæ€§åŒ–å¤„ç†

    Args:
        text: åŸå§‹æ–‡æœ¬
        llm_client: LLM å®¢æˆ·ç«¯
        detected_patterns: å·²æ£€æµ‹åˆ°çš„æ¨¡å¼(å¯é€‰,å¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æ£€æµ‹)

    Returns:
        åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
        - humanized_text: äººæ€§åŒ–åçš„æ–‡æœ¬
        - patterns_fixed: ä¿®å¤çš„æ¨¡å¼åˆ—è¡¨
        - summary: æ›´æ”¹æ‘˜è¦
    """
    # å¦‚æœæœªæä¾›æ£€æµ‹ç»“æœ,å…ˆè¿›è¡Œæ£€æµ‹
    if detected_patterns is None:
        detected_patterns = detect_ai_patterns(text, min_severity="medium")

    if not detected_patterns:
        logger.info("æœªæ£€æµ‹åˆ°éœ€è¦ä¿®å¤çš„ AI æ¨¡å¼")
        return {
            "humanized_text": text,
            "patterns_fixed": [],
            "summary": "æœªæ£€æµ‹åˆ° AI æ¨¡å¼,æ— éœ€ä¿®æ”¹",
        }

    # æ„å»º humanizer prompt
    skill_prompt = _build_humanizer_prompt()

    # æ„å»ºæ£€æµ‹ç»“æœæ‘˜è¦
    patterns_summary = "\n".join(
        [
            f"- {p['pattern_name']} ({p['severity']}): å‡ºç° {p['count']} æ¬¡"
            for p in detected_patterns[:10]  # æœ€å¤šåˆ—å‡º10ä¸ª
        ]
    )

    # æ„å»ºå®Œæ•´ prompt
    full_prompt = f"""{skill_prompt}

## æ£€æµ‹åˆ°çš„ AI æ¨¡å¼

{patterns_summary}

## éœ€è¦äººæ€§åŒ–çš„æ–‡æœ¬

{text}

## è¯·æä¾›

1. äººæ€§åŒ–åçš„æ–‡æœ¬(ç§»é™¤ä¸Šè¿° AI æ¨¡å¼,ä½¿å…¶æ›´è‡ªç„¶)
2. ç®€è¦è¯´æ˜ä¸»è¦æ›´æ”¹
"""

    try:
        # è°ƒç”¨ LLM
        response = llm_client.chat(messages=[{"role": "user", "content": full_prompt}])

        # è§£æå“åº”
        # ç®€å•å®ç°:å‡è®¾ LLM è¿”å›çš„å°±æ˜¯äººæ€§åŒ–åçš„æ–‡æœ¬
        # æ›´å¤æ‚çš„å®ç°å¯ä»¥è¦æ±‚ LLM è¿”å› JSON æ ¼å¼

        return {
            "humanized_text": response,
            "patterns_fixed": detected_patterns,
            "summary": f"ä¿®å¤äº† {len(detected_patterns)} ç§ AI å†™ä½œæ¨¡å¼",
        }

    except Exception as e:
        logger.error(f"æ–‡æœ¬äººæ€§åŒ–å¤±è´¥: {e}", exc_info=True)
        return {
            "humanized_text": text,
            "patterns_fixed": [],
            "summary": f"äººæ€§åŒ–å¤±è´¥: {str(e)}",
        }


def calculate_humanization_score(detected_patterns: List[Dict[str, Any]]) -> int:
    """
    æ ¹æ®æ£€æµ‹åˆ°çš„æ¨¡å¼è®¡ç®—äººæ€§åŒ–å¾—åˆ†

    Args:
        detected_patterns: æ£€æµ‹åˆ°çš„æ¨¡å¼åˆ—è¡¨

    Returns:
        äººæ€§åŒ–å¾—åˆ† (0-20åˆ†)
    """
    if not detected_patterns:
        return 20  # æ»¡åˆ†

    # æ ¹æ®ä¸¥é‡ç¨‹åº¦å’Œæ•°é‡è®¡ç®—æ‰£åˆ†
    severity_weights = {"low": 0.5, "medium": 1.0, "high": 2.0}

    total_penalty = 0
    for pattern in detected_patterns:
        severity = pattern.get("severity", "low")
        count = pattern.get("count", 0)
        weight = severity_weights.get(severity, 1.0)

        # æ¯ä¸ªæ¨¡å¼çš„æ‰£åˆ† = æƒé‡ * min(count, 5)
        # é™åˆ¶å•ä¸ªæ¨¡å¼æœ€å¤šæ‰£ 5 * weight åˆ†
        penalty = weight * min(count, 5)
        total_penalty += penalty

    # æ€»åˆ† 20 åˆ†,æ‰£åˆ†ä¸Šé™ 20 åˆ†
    score = max(0, 20 - int(total_penalty))

    logger.info(f"äººæ€§åŒ–å¾—åˆ†: {score}/20 (æ£€æµ‹åˆ° {len(detected_patterns)} ç§æ¨¡å¼)")

    return score
